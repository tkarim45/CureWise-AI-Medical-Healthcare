{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0da5316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:zillis_rag:Connected to Zilliz/Milvus at https://in05-ea2907a9e50a525.serverless.gcp-us-west1.cloud.zilliz.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "# --- Configuration ---\n",
    "EMBEDDING_DIMENSION = 768\n",
    "COLLECTION_NAME = \"medical_conversations_rag\"\n",
    "ZILLIZ_URI = os.getenv(\"ZILLIZ_URI\", \"https://in05-ea2907a9e50a525.serverless.gcp-us-west1.cloud.zilliz.com\")\n",
    "ZILLIZ_TOKEN = os.getenv(\"ZILLIZ_TOKEN\", \"b9b45871109db0eb4b3e084c2590479dc8e7d3b0ec126bc80489673fa9d400adec193e3737875fd7a172fe71c574b84c2a756214\")\n",
    "\n",
    "# --- Logging ---\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"zillis_rag\")\n",
    "\n",
    "# --- Connect to Zilliz/Milvus ---\n",
    "connections.connect(uri=ZILLIZ_URI, token=ZILLIZ_TOKEN)\n",
    "logger.info(f\"Connected to Zilliz/Milvus at {ZILLIZ_URI}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f1f3065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:zillis_rag:Collection 'medical_conversations_rag' created.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Collection Schema ---\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.VARCHAR, max_length=64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"qtype\", dtype=DataType.VARCHAR, max_length=32, is_primary=False),\n",
    "    FieldSchema(name=\"Question\", dtype=DataType.VARCHAR, max_length=1024, is_primary=False),\n",
    "    FieldSchema(name=\"Answer\", dtype=DataType.VARCHAR, max_length=65000, is_primary=False),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIMENSION, is_primary=False),\n",
    "    FieldSchema(name=\"chunk_index\", dtype=DataType.INT64, is_primary=False),\n",
    "    FieldSchema(name=\"answer_group_id\", dtype=DataType.VARCHAR, max_length=64, is_primary=False),\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"RAG QnA collection with chunking\")\n",
    "\n",
    "\n",
    "# --- Drop and Create Collection ---\n",
    "if COLLECTION_NAME in utility.list_collections():\n",
    "    Collection(COLLECTION_NAME).drop()\n",
    "collection = Collection(COLLECTION_NAME, schema)\n",
    "logger.info(f\"Collection '{COLLECTION_NAME}' created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de2e94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# --- Chunking utility ---\n",
    "def chunk_text(text, chunk_size):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# --- Prepare chunked data for insertion ---\n",
    "chunk_size = 65000\n",
    "ids = []\n",
    "qtypes = []\n",
    "questions = []\n",
    "answers = []\n",
    "embeds = []\n",
    "chunk_indexes = []\n",
    "group_ids = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    answer = str(row['Answer'])\n",
    "    answer_chunks = chunk_text(answer, chunk_size)\n",
    "    group_id = str(uuid.uuid4())\n",
    "    for chunk_idx, chunk in enumerate(answer_chunks):\n",
    "        ids.append(str(uuid.uuid4()))\n",
    "        qtypes.append(str(row['qtype']))\n",
    "        questions.append(str(row['Question']))\n",
    "        answers.append(chunk)\n",
    "        chunk_indexes.append(chunk_idx)\n",
    "        group_ids.append(group_id)\n",
    "        # Use the same embedding for all chunks of the same answer\n",
    "        # (or recompute for each chunk if you prefer)\n",
    "\n",
    "# Generate embeddings for all questions (repeat for each chunk)\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "question_embeddings = embedding_model.embed_documents(questions)\n",
    "embeds = np.array(question_embeddings, dtype=np.float32)\n",
    "\n",
    "insert_data = [\n",
    "    ids,\n",
    "    qtypes,\n",
    "    questions,\n",
    "    answers,\n",
    "    embeds.tolist(),\n",
    "    chunk_indexes,\n",
    "    group_ids\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17863d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:zillis_rag:Inserted 16407 rows (with chunking) into 'medical_conversations_rag'.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "for start in range(0, len(ids), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = [\n",
    "        ids[start:end],\n",
    "        qtypes[start:end],\n",
    "        questions[start:end],\n",
    "        answers[start:end],\n",
    "        embeds[start:end].tolist(),\n",
    "        chunk_indexes[start:end],\n",
    "        group_ids[start:end]\n",
    "    ]\n",
    "    collection.insert(batch)\n",
    "collection.flush()\n",
    "logger.info(f\"Inserted {len(ids)} rows (with chunking) into '{COLLECTION_NAME}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16ba3026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:zillis_rag:Index created on embedding field.\n",
      "INFO:zillis_rag:Collection loaded for search.\n",
      "INFO:zillis_rag:Collection loaded for search.\n"
     ]
    }
   ],
   "source": [
    "# --- Create index on embedding field ---\n",
    "index_params = {\n",
    "    \"metric_type\": \"L2\",  # or \"COSINE\" if you prefer\n",
    "    \"index_type\": \"IVF_FLAT\",  # or \"HNSW\", \"AUTOINDEX\", etc. depending on your needs\n",
    "    \"params\": {\"nlist\": 1024}\n",
    "}\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "logger.info(\"Index created on embedding field.\")\n",
    "\n",
    "# --- Load Collection for Search ---\n",
    "collection.load()\n",
    "logger.info(\"Collection loaded for search.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7376c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer is a disease where cells grow and multiply uncontrollably, forming a mass called a tumor.  These tumors can be benign (not cancerous) or malignant (cancerous). Malignant tumors can invade nearby tissues and spread to other parts of the body (metastasis).  There are over 100 different types of cancer, most named for where they originate (e.g., lung cancer).  The growth of cancer cells is uncontrolled, unlike normal cells which grow and die in a controlled manner.\n"
     ]
    }
   ],
   "source": [
    "# --- RAG Chain Setup ---\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0.3)\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    **Note:** You are an AI assistant using only the provided context from a dataset of patient-doctor conversations. You are not a doctor. Do not provide medical advice beyond the context. If the context lacks information, say so. Use the conversation history to understand references (e.g., pronouns like 'it') if relevant.\n",
    "\n",
    "    **Conversation History (Recent Queries and Answers):**\n",
    "    {history}\n",
    "\n",
    "    **Context (from dataset):**\n",
    "    {context}\n",
    "\n",
    "    **Question:**\n",
    "    {input}\n",
    "\n",
    "    **Answer (based only on context, using history for clarity if needed):**\n",
    "    If the question is about hospitals, doctors, or appointments, respond: \"Please use the appointment booking system for this query.\"\n",
    "    Otherwise, provide an answer based on the context.\n",
    "    \"\"\"\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt_template)\n",
    "\n",
    "# --- Retrieval Function ---\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def retrieve_context(query, top_k=3):\n",
    "    query_emb = embedding_model.embed_query(query)\n",
    "    results = collection.search(\n",
    "        data=[query_emb],\n",
    "        anns_field=\"embedding\",\n",
    "        param={\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}},\n",
    "        limit=top_k,\n",
    "        output_fields=[\"qtype\", \"Question\", \"Answer\", \"chunk_index\", \"answer_group_id\"]\n",
    "    )\n",
    "    docs = []\n",
    "    for hits in results:\n",
    "        for hit in hits:\n",
    "            docs.append(Document(\n",
    "                page_content=getattr(hit.entity, \"Answer\", \"\"),\n",
    "                metadata={\n",
    "                    \"qtype\": getattr(hit.entity, \"qtype\", \"\"),\n",
    "                    \"Question\": getattr(hit.entity, \"Question\", \"\"),\n",
    "                    \"chunk_index\": getattr(hit.entity, \"chunk_index\", 0),\n",
    "                    \"answer_group_id\": getattr(hit.entity, \"answer_group_id\", \"\")\n",
    "                }\n",
    "            ))\n",
    "    return docs\n",
    "\n",
    "# --- RAG QA Function ---\n",
    "def rag_qa(question, history=\"\"):\n",
    "    context = retrieve_context(question)\n",
    "    input_vars = {\n",
    "        \"history\": history,\n",
    "        \"context\": context,  # pass as list of Document\n",
    "        \"input\": question\n",
    "    }\n",
    "    return document_chain.invoke(input_vars)\n",
    "\n",
    "# --- Example Usage ---\n",
    "response = rag_qa(\"What is cancer?\")\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55a0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
